{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.functional as Func\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "\n",
    "from functions.functions_for_dataload import *\n",
    "from functions.fnc import *\n",
    "\n",
    "######### Generating new samples ###########\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "\n",
    "today = date.today()\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "numOfBuses = 14\n",
    "numOfLines = 20\n",
    "attacked_Bus = 5\n",
    "\n",
    "num_Of_attacked_buses = attacked_Bus\n",
    "\n",
    "noise_dimension = numOfBuses\n",
    "z_dim = noise_dimension\n",
    "numOfZ = numOfBuses + 2* numOfLines\n",
    "numOfStates = numOfBuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here cluster and substation are used interchangeably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f\"Bus Data\\\\IEEE_{numOfBuses}.xlsx\"\n",
    "\n",
    "# Load data into Dataframes\n",
    "bus_data_df = pd.read_excel (file_name, sheet_name = \"Bus\")\n",
    "line_data_df = pd.read_excel (file_name, sheet_name = \"Branch\")\n",
    "# update the index from 1 to number of elements\n",
    "bus_data_df.set_index(pd.Series(range(1, numOfBuses+1)), inplace = True)\n",
    "line_data_df.set_index(pd.Series(range(1, numOfLines+1)), inplace = True)\n",
    "\n",
    "# number of lines and measurements\n",
    "numOfLines = line_data_df.shape[0]\n",
    "numOfZ = numOfBuses + numOfLines * 2\n",
    "W_list = (numOfZ + 1)*[1]\n",
    "\n",
    "############################################################\n",
    "# Loading Topology Data and Measurement Data\n",
    "try:\n",
    "    topo_mat = pd.read_excel(file_name, sheet_name = \"Topology Matrix\")\n",
    "    line_data  = pd.read_excel(file_name, sheet_name = \"Line Data\")\n",
    "    print(\"Topology Matrix Loaded!\")\n",
    "except:\n",
    "    print(\"Generating Topology Matrix...\")\n",
    "    topo_mat, line_data = generate_topology_matrix(numOfBuses, numOfLines, line_data_df, file_name)\n",
    "\n",
    "Topo = line_data.values.astype(int) #Another name\n",
    "###############################################################\n",
    "# Loading Topology Data and Measurement Data\n",
    "try:\n",
    "    Z_msr_org = pd.read_excel(file_name, sheet_name = \"Measurement Data\")\n",
    "    bus_data = pd.read_excel(file_name, sheet_name = \"Bus Data\")\n",
    "    print(\"Measurement Data Loaded!\")\n",
    "except:\n",
    "    print(\"Generating Measurement Data...\")\n",
    "    Z_msr_org, bus_data = generate_Z_msr_org(numOfBuses, numOfLines, bus_data_df, topo_mat, file_name)\n",
    "\n",
    "# Adding IDs and Reported columns\n",
    "Z_msr_org.insert(0, 'ID', list(Z_msr_org.index.values))\n",
    "Z_msr_org.insert(1, 'Reported', [1]* (numOfZ+1))\n",
    "###############################################################\n",
    "#************  reading network topo data **************\n",
    "fnetname = f'Bus Data//Net_Topo_{numOfBuses}.txt'\n",
    "try: netdata = open(fnetname).readlines() \n",
    "except: \n",
    "    print(\"File Does not exist!\")\n",
    "################################################################\n",
    "\n",
    "# Load Attack Data, otherwise generate attack data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_others(numOfBuses, numOfLines, num_Of_attacked_buses, data, Topo):\n",
    "    data = pd.DataFrame(data)\n",
    "    numOfZ = numOfBuses + 2* numOfLines\n",
    "    num_Of_attack = int (data.shape[0]/numOfZ)\n",
    "    # Preprocessing data\n",
    "\n",
    "    false_Data = pd.DataFrame([])\n",
    "    combination_Data_Sensor = pd.DataFrame([])\n",
    "    combination_Data_Cluster = pd.DataFrame([])\n",
    "\n",
    "    start_indx = 0\n",
    "    end_indx = start_indx + numOfZ + 1\n",
    "\n",
    "    progress = tqdm.tqdm(total=num_Of_attack, desc='Done', position=0)\n",
    "    \n",
    "    while (end_indx <= data.shape[0]):\n",
    "        progress.update(1)\n",
    "        attack_data = data.iloc[start_indx + 1 : end_indx, 2]\n",
    "        combination_data = data.iloc[start_indx + 1 : end_indx, 1].astype(int)\n",
    "\n",
    "        sensor_ids = np.where(combination_data.values == 1)[0]+1\n",
    "        \n",
    "        if len(sensor_ids) > 0:        \n",
    "            sensor_ids_list = []\n",
    "            sensor_ids_list.append(sensor_ids)\n",
    "            \n",
    "            cluster_maping =  assignClusterID_v2 (sensor_ids_list, Topo, NumOfBuses=numOfBuses, NumOfLines = numOfLines)\n",
    "            combination_Data_Cluster = combination_Data_Cluster.append(pd.DataFrame(cluster_maping, index = None, columns = None))\n",
    "\n",
    "            false_Data = false_Data.append(pd.DataFrame(attack_data.values, index = None, columns = None).T)\n",
    "            combination_Data_Sensor = combination_Data_Sensor.append(pd.DataFrame(combination_data.values, index = None, columns = None).T)\n",
    "\n",
    "        start_indx += numOfZ+1\n",
    "        end_indx = start_indx + numOfZ + 1\n",
    "        #print(start_indx, end_indx)\n",
    "    false_Data.index = range(1, false_Data.shape[0]+1)\n",
    "    false_Data.columns = range(1, false_Data.shape[1]+1)\n",
    "    combination_Data_Sensor.index = range(1, combination_Data_Sensor.shape[0]+1)\n",
    "    combination_Data_Sensor.columns = range(1, combination_Data_Sensor.shape[1]+1)\n",
    "\n",
    "    combination_Data_Cluster.index = range(1, combination_Data_Cluster.shape[0]+1)\n",
    "    combination_Data_Cluster.columns = range(1, combination_Data_Cluster.shape[1]+1)\n",
    "\n",
    "\n",
    "    false_Data.to_excel(f'Attack Data//false_Data_{numOfBuses}_{numOfLines}_{num_Of_attacked_buses}.xlsx', engine='xlsxwriter', index = True) \n",
    "    combination_Data_Sensor.to_excel(f'Attack Data//combination_Data_Sensor_{numOfBuses}_{numOfLines}_{num_Of_attacked_buses}.xlsx', engine='xlsxwriter', index = True) \n",
    "    combination_Data_Cluster.to_excel(f'Attack Data//combination_Data_Cluster_{numOfBuses}_{numOfLines}_{num_Of_attacked_buses}.xlsx', engine='xlsxwriter', index = True) \n",
    "    return false_Data, combination_Data_Sensor, combination_Data_Cluster\n",
    "\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "def assignClusterID_v2 (sensorList, Topo, NumOfBuses, NumOfLines):\n",
    "    import numpy as np\n",
    "    \n",
    "    clusterList = []\n",
    "    #Each sensor list\n",
    "    for idx, sensors in enumerate(sensorList):\n",
    "        #  Each sensor\n",
    "        \n",
    "        clusters = []\n",
    "        if len(sensors) == 0:\n",
    "            clusterList.append([0]*numOfBuses)\n",
    "#             print(\"Continue\", idx, sensors)\n",
    "            continue\n",
    "        for sensorID in sensors:\n",
    "            if sensorID > 2 * NumOfLines:\n",
    "                clusterID = sensorID - 2 * NumOfLines\n",
    "\n",
    "            elif sensorID > NumOfLines:\n",
    "                lineID = sensorID - NumOfLines\n",
    "                clusterID = Topo[lineID-1, 2]\n",
    "\n",
    "            elif sensorID > 0:\n",
    "                clusterID = Topo[sensorID-1, 1] \n",
    "            \n",
    "            clusters.append(clusterID)\n",
    "\n",
    "        clusterID = np.unique(np.array(clusters))\n",
    "#         print(clusterID)\n",
    "        clusterID_Enc = np.zeros((NumOfBuses), dtype = int)\n",
    "        clusterID_Enc[clusterID-1] = 1\n",
    "    \n",
    "        clusterList.append(clusterID_Enc)\n",
    "    \n",
    "    return np.array(clusterList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensubDict = {}\n",
    "x = assignClusterID (np.arange(1,numOfZ+1), Topo, numOfBuses, numOfLines)[0]\n",
    "for i in range(1,numOfBuses+1):\n",
    "    sensubDict[i] = np.where(x == i)[0]+1\n",
    "# sensubDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Attack Data, otherwise generate attack data\n",
    "try:\n",
    "    file_Name_ = \"Attack_Space_\"+str(numOfBuses)+\"_\"+str(numOfLines)+\"_\"+str(attacked_Bus)+\".csv\"\n",
    "    Attack_Data = np.genfromtxt(\"Attack Data//\"+file_Name_, delimiter=',')\n",
    "    print(\"Attack data loaded!\")\n",
    "    file_name = f\"IEEE_{numOfBuses}.xlsx\"\n",
    "    false_Data = pd.read_excel(f'Attack Data//false_Data_{numOfBuses}_{numOfLines}_{attacked_Bus}.xlsx', index_col=0)\n",
    "    combination_Data_Sensor = pd.read_excel(f'Attack Data//combination_Data_Sensor_{numOfBuses}_{numOfLines}_{num_Of_attacked_buses}.xlsx', index_col=0)\n",
    "    combination_Data_Cluster = pd.read_excel(f'Attack Data//combination_Data_Cluster_{numOfBuses}_{numOfLines}_{num_Of_attacked_buses}.xlsx', index_col=0)\n",
    "    # combination_Data = pd.read_excel(f'Attack Data//combination_Data_{numOfBuses}_{numOfLines}_{attacked_Bus}.xlsx', index_col=0)\n",
    "\n",
    "except:\n",
    "    print(\"Attack Data is missing! Generating attack data!\")\n",
    "    current_path = os.getcwd()\n",
    "    Attack_Data = generate_attackdata(numOfBuses, numOfLines, line_data, attacked_Bus, current_path)\n",
    "    \n",
    "    false_Data, combination_Data_Sensor, combination_Data_Cluster = generate_others(\n",
    "        numOfBuses, numOfLines, attacked_Bus, Attack_Data, Topo)\n",
    "    \n",
    "numOfAttacks = int (Attack_Data.shape[0]/(numOfZ+1))\n",
    "print(\"numOfAttacks: \", numOfAttacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_type = 'Sensor' # or \"Cluster\"\n",
    "\n",
    "if cond_type == 'Sensor':\n",
    "    Cond_Data = combination_Data_Sensor\n",
    "    c_dim = numOfZ\n",
    "elif cond_type == 'Cluster':\n",
    "    Cond_Data = combination_Data_Cluster\n",
    "    c_dim = numOfBuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cond_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Taking 100 sample for training\n",
    "\n",
    "#################################  Test Size #########################\n",
    "test_size = 0.25\n",
    "######################################################################\n",
    "\n",
    "#splitting train and test data\n",
    "false_Data_Train, false_Data_Test = train_test_split(false_Data, test_size = test_size, random_state=42)\n",
    "X_Train, X_Test, Cond_Data_Train, Cond_Data_Test = train_test_split(false_Data, Cond_Data, test_size = test_size, random_state=42)\n",
    "\n",
    "# combination_Data_Train, combination_Data_Test =  train_test_split(combination_Data, test_size = test_size, random_state=42)\n",
    "# Scaling the data\n",
    "stdscaler = StandardScaler()\n",
    "X_Train = stdscaler.fit_transform(X_Train) \n",
    "\n",
    "\n",
    "print(\"Split done!\")\n",
    "print(\"Total training data: \", false_Data_Train.shape[0])\n",
    "\n",
    "\n",
    "data = [(x_i, y_i) for x_i, y_i in zip(X_Train, Cond_Data_Train.values)]\n",
    "class Data_Loader():\n",
    "    \n",
    "    def __init__(self,data_list):       \n",
    "        self.data=data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index][0]\n",
    "        sample_tensor = Tensor(sample).float()\n",
    "        label = self.data[index][1]\n",
    "        label_tensor = Tensor(label).float()\n",
    "        return (sample_tensor, label_tensor)\n",
    "\n",
    "dataSet=Data_Loader(data)\n",
    "data_loader = DataLoader(dataset=dataSet, batch_size = 64, shuffle= True)\n",
    "print(\"Data Loader Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = \"CWGAN_WC\"\n",
    "# Model = \"GAN\"\n",
    "model_Train = True\n",
    "\n",
    "if cond_type == 'Sensor':\n",
    "    if numOfBuses == 14:\n",
    "        G_nodes = [z_dim+c_dim, 64, 60, numOfZ]\n",
    "        D_nodes = [numOfZ+c_dim, 64, 32, 1]\n",
    "        \n",
    "    if numOfBuses == 30:\n",
    "        G_nodes = [z_dim+c_dim, 130, 120, numOfZ]\n",
    "        D_nodes = [numOfZ+c_dim, 100, 50, 1]\n",
    "\n",
    "elif cond_type == 'Cluster':\n",
    "    if numOfBuses == 14:\n",
    "        G_nodes = [z_dim+c_dim, 32, 48, numOfZ]\n",
    "        D_nodes = [numOfZ+c_dim, 32, 16, 1]\n",
    "        \n",
    "    if numOfBuses == 30:\n",
    "        G_nodes = [z_dim+c_dim, 70, 80, numOfZ]\n",
    "        D_nodes = [numOfZ+c_dim, 68, 32, 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Model == 'CWGAN_WC':\n",
    "    # GeneratorNet\n",
    "    G = torch.nn.Sequential(\n",
    "        nn.Linear(G_nodes[0], G_nodes[1]),\n",
    "        nn.ReLU(True),\n",
    "    #     nn.Dropout(0.10),\n",
    "\n",
    "        nn.Linear(G_nodes[1], G_nodes[2]),\n",
    "        nn.ReLU(True),\n",
    "    #     nn.Dropout(0.10),\n",
    "        nn.Linear(G_nodes[2], G_nodes[3])\n",
    "    )\n",
    "\n",
    "    # DiscriminatorNet\n",
    "    D = torch.nn.Sequential(\n",
    "        nn.Linear(D_nodes[0], D_nodes[1]),\n",
    "        nn.ReLU(True),\n",
    "    #     nn.Dropout(0.15),\n",
    "\n",
    "        nn.Linear(D_nodes[1], D_nodes[2]),\n",
    "        nn.ReLU(True),\n",
    "    #     nn.Dropout(0.15),\n",
    "\n",
    "        nn.Linear(D_nodes[2], D_nodes[3])\n",
    "    #     nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "\n",
    "    # reset_grad\n",
    "    def reset_grad():\n",
    "        G.zero_grad()\n",
    "        D.zero_grad()\n",
    "\n",
    "    def noise(size):\n",
    "        n = torch.randn(size, z_dim)\n",
    "        return n\n",
    "    \n",
    "    def train_discriminator(optimizer, real_data, fake_data):\n",
    "#         print(\"train_discriminator\")\n",
    "        # predict on the real and fake data\n",
    "        D_real = D(real_data)\n",
    "        D_fake = D(fake_data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        D_loss = -(torch.mean(D_real) - torch.mean(D_fake))\n",
    "\n",
    "        D_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Weight clipping\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "#             #reset gradient\n",
    "        reset_grad()\n",
    "        # Return error and predictions for real and fake inputs\n",
    "        return D_loss\n",
    "\n",
    "    def train_generator(optimizer, fake_data):\n",
    "        # Reset gradients\n",
    "#         reset_grad()\n",
    "        # Sample noise and generate fake data\n",
    "        D_fake = D(fake_data)\n",
    "        # Calculate error and backpropagate\n",
    "        G_loss = -torch.mean(D_fake)\n",
    "        G_loss.backward()\n",
    "        # Update weights with gradients\n",
    "        optimizer.step()\n",
    "        # Return error\n",
    "        reset_grad()\n",
    "        return G_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_harmonizer(optimizer, sample):\n",
    "    sample = stdscaler.inverse_transform(sample.data)\n",
    "    sample_est = H_tensor@(torch.pinverse(H_tensor.T@H_tensor)@H_tensor.T)@sample.reshape(numOfZ,1).flatten()\n",
    "    sample = torch.from_numpy(sample.flatten())\n",
    "    sample = Variable(sample.data, requires_grad=True)\n",
    "    sample_est = Variable(sample_est.data, requires_grad=True)\n",
    "    G_loss_MSE = alpha * MSE_loss(sample_est, sample)\n",
    "    G_loss_MSE.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return G_loss_MSE\n",
    "\n",
    "def noise(size):\n",
    "    '''\n",
    "    Generates a 1-d vector of gaussian sampled random values\n",
    "    '''\n",
    "    n = torch.randn(size, noise_dimension)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model():\n",
    "    print(\"epoch: \", epoch,'\\n\\n')\n",
    "    ######\n",
    "    # Ploting the laerning curve\n",
    "    plt.close()\n",
    "    plt.figure(0)\n",
    "    plt.plot(d_errs, color = 'r')\n",
    "    plt.plot(g_errs, color = 'g')\n",
    "    plt.show()\n",
    "    # plotting correlation\n",
    "    \n",
    "def save_model():\n",
    "    time = str(datetime.now()).split(\".\")[0]\n",
    "    time = time.replace(\":\",\"-\")\n",
    "\n",
    "    torch.save(G, f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_generator_{epoch}_'+ str(time) + \".pb\")\n",
    "    torch.save(D, f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_discriminator_{epoch}_'+ str(time) + \".pb\")\n",
    "\n",
    "    #Saving the learning curve\n",
    "    plt.figure(0)\n",
    "    plt.plot(d_errs, color = 'r', label= 'd_errs')\n",
    "    plt.plot(g_errs, color = 'g', label= 'g_errs')\n",
    "    plt.xlabel(\"Number of epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training curve of {Model}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_Learning_Curve_{epoch}_' + str(time) +'.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################    Copying the original Data ###########################\n",
    "print('Checking State Estimation and Bad Data Detector on clean data') \n",
    "H_org = topo_mat.values.copy() \n",
    "Z_org = Z_msr_org.values.copy()\n",
    "Z_mat = Z_org.copy()\n",
    "Threshold_min = 2\n",
    "Threshold_max = 2\n",
    "W_list = (numOfZ + 1)*[1]\n",
    "##################################################################################\n",
    "# Running the State Estimation and Bad Data Detection Algorithm on actual data\n",
    "States_init, Z_est_init, Z_mat_init, M_Noise_actu, Noisy_index_actu, fullRank, Threshold = SE_BDD_(\n",
    "    H_org.copy(), Z_org.copy(), W_list, Threshold_min, Threshold_max, Verbose = \"False\")\n",
    "if Noisy_index_actu.size == 0:\n",
    "    print(\"Working fine!\")\n",
    "else: print(\"Got some issue!!!!!!\")\n",
    "    \n",
    "H_tensor = torch.from_numpy(H_org[1:,:]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "MSE_loss = nn.MSELoss()\n",
    "lr = 5e-5\n",
    "disRepaet = 5\n",
    "max_harmonize = 32\n",
    "# max_harmonize = 0\n",
    "alpha = lr\n",
    "\n",
    "\n",
    "#List to save the loss\n",
    "g_errs = []\n",
    "d_errs = []\n",
    "mse_errs = []\n",
    "loss = nn.BCELoss()\n",
    "num_epochs = 25000\n",
    "epoch = num_epochs - 1\n",
    "\n",
    "if Model == 'CWGAN_WC' and model_Train == True:\n",
    "    #Hyper parameters\n",
    "\n",
    "    \n",
    "    \n",
    "    # Optimizer functions\n",
    "    g_optimizer = optim.RMSprop(G.parameters(), lr = lr, momentum= 0.9)\n",
    "    d_optimizer = optim.RMSprop(D.parameters(), lr = lr, momentum= 0.9)\n",
    "    \n",
    "    progress = tqdm.tqdm(total=num_epochs, desc='Epoch', position = 0)\n",
    "\n",
    "    # starting the epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        #print(\"Epoch: \", epoch)\n",
    "        progress.update(1)\n",
    "         \n",
    "        # Starting Discriminator training\n",
    "        for repeat in range(disRepaet):\n",
    "            for n_batch, batch in enumerate(data_loader):\n",
    "                \n",
    "                ######## Creating combined batch #########\n",
    "                # Splitting the data into real data and condition\n",
    "                real_batch = batch[0]\n",
    "                cond_batch = batch[1]\n",
    "                \n",
    "                # Creating input for generator with condition\n",
    "                N = real_batch.size(0) \n",
    "                noise_batch = noise(N)\n",
    "                \n",
    "                g_inputs = torch.cat([noise_batch, cond_batch], 1)\n",
    "\n",
    "                #Train Discriminator\n",
    "                # Generate fake data and detach \n",
    "                fake_data = G(g_inputs).detach()\n",
    "                \n",
    "                # Creating input for discriminator with condition\n",
    "                d_inputs_real = torch.cat([real_batch, cond_batch], 1)\n",
    "                d_inputs_fake = torch.cat([fake_data, cond_batch], 1)\n",
    "                \n",
    "                \n",
    "                # Train D\n",
    "                D_loss = train_discriminator(d_optimizer, d_inputs_real, d_inputs_fake)\n",
    "\n",
    "        # Starting Generator training\n",
    "        for n_batch, batch in enumerate(data_loader):\n",
    "                \n",
    "            ######## Creating combined batch #########\n",
    "            # Splitting the data into real data and condition\n",
    "            real_batch = batch[0]\n",
    "            cond_batch = batch[1]\n",
    "            \n",
    "            # Creating input for generator with condition\n",
    "            N = real_batch.size(0) \n",
    "            noise_batch = noise(N)\n",
    "            g_inputs = torch.cat([noise_batch, cond_batch], 1)\n",
    "\n",
    "            # Generate fake data\n",
    "            fake_data = G(g_inputs)\n",
    "            d_inputs = torch.cat([fake_data, cond_batch], 1)\n",
    "\n",
    "\n",
    "            # Train G\n",
    "            G_loss = train_generator(g_optimizer, d_inputs)\n",
    "        \n",
    "        \n",
    "#         #Starting harmonizer training\n",
    "# #         if epoch > 0 :\n",
    "#         for harmonize_repeat in range(max_harmonize):\n",
    "#             fake_data = G(noise(1))\n",
    "#             G_loss_MSE = train_harmonizer(g_optimizer, fake_data)\n",
    "#             mse_errs.append(G_loss_MSE)       \n",
    "\n",
    "\n",
    "        d_errs.append(D_loss)\n",
    "        g_errs.append(G_loss)\n",
    "        \n",
    "        if (epoch+1) % 10000 == 0:\n",
    "            # show and eval model generating a single sample\n",
    "            eval_model()\n",
    "            save_model()\n",
    "    ##################### Saving the model #######\n",
    "    #save_model()\n",
    "    ####################################################\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# # importing the latest models\n",
    "else:\n",
    "    Models_list = glob.glob(f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_*_{epoch}_*.pb')\n",
    "    try:\n",
    "        Models_list.sort(reverse= True)\n",
    "        latest_time = Models_list[0].split(\"_\")[-1][0:-3]\n",
    "        print(\"latest_time:\", latest_time)\n",
    "        print(f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_generator_{epoch}_'+ str(latest_time) + \".pb\")\n",
    "        G = torch.load(f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_generator_{epoch}_'+ str(latest_time) + \".pb\")\n",
    "        D = torch.load(f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_discriminator_{epoch}_'+ str(latest_time) + \".pb\")\n",
    "        print(\"Model Loaded\")\n",
    "    except:\n",
    "        print(\"Model not found!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considered data\n",
    "eval_type = 'Train'\n",
    "\n",
    "th = 0\n",
    "\n",
    "if eval_type == 'Train':\n",
    "    cond_sample = Cond_Data_Train.values\n",
    "    \n",
    "elif eval_type  == 'Test':\n",
    "    cond_sample = Cond_Data_Test.values\n",
    "\n",
    "def model_predict(th, cond_sample):\n",
    "    print(\"Generating attack vector!\")\n",
    "    df =pd.DataFrame([]).astype(int)\n",
    "    \n",
    "    cond_batch = torch.from_numpy(cond_sample).float()  \n",
    "    \n",
    "    N = cond_batch.shape[0]\n",
    "    noise_batch = noise(N)\n",
    "    \n",
    "    g_inputs = torch.cat([noise_batch, cond_batch], 1)\n",
    "\n",
    "        \n",
    "    # Generate fake data\n",
    "    test_images = G(g_inputs).data\n",
    "\n",
    "    # test_images = G(noise(1)).data\n",
    "    sample = stdscaler.inverse_transform(test_images)\n",
    "    sample= np.squeeze(sample).astype(int)\n",
    "    sample[abs(sample)< th] = 0\n",
    "    return sample\n",
    "\n",
    "samples = model_predict(th, cond_sample)\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "def attack_estimate(H_mat, Z_mat):\n",
    "    \n",
    "    import numpy as np\n",
    "    from numpy.random import seed\n",
    "    from numpy.random import randn\n",
    "    from numpy import mean\n",
    "    from numpy import std\n",
    "    \n",
    "   \n",
    "    #print(\"\\n **************  State Estimation ************* \\n\")\n",
    "    #takes H matrix Z measurements, Threshold and number of Equations as input\n",
    "    #print(\"Considering only the taken measurements\")\n",
    "    Z_msr_init = Z_mat[Z_mat[:,1]==1][:,2]\n",
    "    \n",
    "    # considering only the corresponding columns in H   \n",
    "    H_mat_init = H_mat[Z_mat[:,1]==1]\n",
    "\n",
    "    #printing the sizes of H, Rank (H) and Z\n",
    "    Rank = np.linalg.matrix_rank(H_mat_init)\n",
    "    #print(\"Rank of Noisy: \" , Rank)\n",
    "    \n",
    "\n",
    "    \n",
    "    if Rank == H_mat.shape[1]:\n",
    "        \n",
    "        fullRank = True\n",
    "        # Estimating the states using WMSE estimator\n",
    "        #States_init = np.linalg.pinv(H_mat_init)@Z_msr_init\n",
    "        States_init = (np.linalg.inv(H_mat_init.T@H_mat_init)@H_mat_init.T)@Z_msr_init\n",
    "\n",
    "        # Estimating the measurement from the estimated states\n",
    "        Z_est = H_mat@States_init\n",
    "\n",
    "    else:\n",
    "        print(\"The H is not a full rank matrix !!\")\n",
    "        fullRank = False\n",
    "        Z_est = np.zeros(H_mat.shape[0])\n",
    "        States_init = np.zeros(H_mat.shape[1])\n",
    "        P_Noise = 0\n",
    "        \n",
    "    return States_init, Z_est, fullRank\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cond_type == 'Sensor':\n",
    "    csenData = cond_sample\n",
    "else:\n",
    "    ########################\n",
    "    csubdata = cond_sample.copy()\n",
    "    csenData = samples*0\n",
    "    csenData.shape\n",
    "    #%%%%%%%%%%%%%%%%\n",
    "    for raw, sub in enumerate (csubdata):\n",
    "        for col, s in enumerate (sub):\n",
    "            if s == 1:\n",
    "                csenData[raw, sensubDict[col+1]-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Updated\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "Threshold_min = 2\n",
    "Threshold_max = 2\n",
    "\n",
    "# Generated images\n",
    "samples = model_predict(th, cond_sample)\n",
    "\n",
    "# Sdt factors\n",
    "std_factors = [0.00, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "max_est_list = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# std_factors = [0.00, 0.25]\n",
    "# max_est_list = [0, 1]\n",
    "\n",
    "run_harmonizer = False\n",
    "\n",
    "total_noisy_preds = []\n",
    "total_distance = []\n",
    "success_rates = []\n",
    "false_Data_Gen_list = []\n",
    "raw_Data_Gen_list = []\n",
    "combination_Data_Gen_list = []\n",
    "data_df_list = []\n",
    "noisy_preds = []\n",
    "\n",
    "distance = []\n",
    "\n",
    "final_Data = pd.DataFrame([])\n",
    "false_Data_Gen_Est = []\n",
    "cond_Data_Gen_Est = []\n",
    "\n",
    "\n",
    "for max_est in max_est_list:\n",
    "    print(\"Max Estim:\", max_est)\n",
    "    success = [0]*len(std_factors)\n",
    "    false_Data_Gen = pd.DataFrame([])\n",
    "    cond_Data_Gen = pd.DataFrame([])\n",
    "    data_df=  pd.DataFrame([])\n",
    "    \n",
    "    for index, (condition, sample_init) in enumerate(zip(csenData, samples)):\n",
    "        #print(\"\\n\\nsample ID: \", index)\n",
    "        #print(\"Generating Sample:\")\n",
    "        \n",
    "        sample_init[np.logical_not(condition.astype(bool))] = 0\n",
    "        sample_zero = sample_init.copy()\n",
    "        ###### save the  raw data #######\n",
    "        data_df[f'r_for_{index}'] = sample_init[0:20]\n",
    "        data_df[f'r_bac_{index}'] = sample_init[20:40]\n",
    "        data_df[f'r_bus_{index}'] = pd.Series(sample_init[40:].tolist())\n",
    "\n",
    "        # Saving the raw data\n",
    "        column_name = f'{index}_0'\n",
    "        sample_df = pd.DataFrame(sample_init.tolist(), index = range(1, numOfZ+1), columns = [column_name])\n",
    "        #false_Data_Gen = false_Data_Gen.append(sample_df.T,  ignore_index = False)\n",
    "\n",
    "        ###########################\n",
    "        sample_mean = np.mean(sample_init)\n",
    "        sample_std = np.std(sample_init)\n",
    "        #print(\"Mean and Std:\", sample_mean, sample_std)\n",
    "\n",
    "        # repeating for delZ_th_list\n",
    "        for std_index, std_fact in enumerate(std_factors):\n",
    "            \n",
    "            dict_data = {}\n",
    "            #print(\"\\nstd_fact: \", std_fact)\n",
    "            #setting up the threshold\n",
    "            delZ_th = sample_std*std_fact\n",
    "            #print(\"std_fact, delZ_th :\", std_fact, delZ_th)\n",
    "            \n",
    "            # Repeating for muliple times\n",
    "            for est in range(max_est+1):\n",
    "\n",
    "                #print(\"Estimation: \", est, max_est)\n",
    "\n",
    "                # Senitize the samples\n",
    "#                 if run_harmonizer == True:\n",
    "                if max_est > 0:\n",
    "                    Z_mat[1:, 2] = sample_init.copy()\n",
    "                    States_org, Z_est_org, fullRank_org = attack_estimate(H_org.copy(), Z_mat.copy())\n",
    "                    sample = Z_est_org[1:].copy()\n",
    "            #         if est == 0:\n",
    "                    dis = mean_absolute_error(sample_zero, Z_est_org[1:])\n",
    "                        #dis  = np.mean(abs(sample_zero - Z_est_org[1:])) #/np.linalg.norm(Z_est_org[1:])*100\n",
    "                else:\n",
    "                    sample = sample_init.copy()\n",
    "                    dis = 0\n",
    "\n",
    "                # Removing noises\n",
    "                #sample[abs(sample)<1] = 0\n",
    "                \n",
    "                #removing unaccessible injection\n",
    "                #*****************************************************************************\n",
    "                sample[np.logical_not(condition.astype(bool))] = 0\n",
    "                \n",
    "                # Replacing with 0\n",
    "                sample[abs(sample)<delZ_th] = 0\n",
    "\n",
    "                ### Verifing the attack stealthyness\n",
    "                Z_mat[1:, 2] = Z_org[1:, 2] + sample\n",
    "                W_list = (numOfZ + 1)*[1]\n",
    "\n",
    "                # estimate the attack vectors\n",
    "                try:\n",
    "                    States_init, Z_est_init, Z_mat_init, M_Noise_actu, Noisy_index_actu, fullRank, Threshold = SE_BDD_(\n",
    "                    H_org.copy(), Z_mat.copy(), W_list, Threshold_min , Threshold_max , Verbose = \"False\")\n",
    "                    #print(f\"\\nStep: {est}: Noise: \", Noisy_index_actu, \"Threshold:\", Threshold, \"fullRank:\", fullRank)\n",
    "    #                 #save the noisy sensors in the fist estimation \n",
    "    #                 if est == 0: noisy_preds.append(Noisy_index_actu.size)  \n",
    "\n",
    "                except:\n",
    "                    print(\"Error in estiamtion\")\n",
    "                    break\n",
    "\n",
    "\n",
    "                #final_sample = sample.copy()\n",
    "\n",
    "                if  fullRank == False or np.linalg.norm(sample[0:2*numOfLines]) == 0: #abs(final_sample).max()> 1000 or\n",
    "#                     print(f\"\\nStep: {est}: Noise: \", Noisy_index_actu, \"Threshold:\", Threshold, \"fullRank:\", fullRank)\n",
    "#                     print(\"abs(sample).max()\", abs(sample).max(), sample.argmax())\n",
    "                    break\n",
    "\n",
    "                    ###########################\n",
    "\n",
    "                if Noisy_index_actu.size == 0:\n",
    "\n",
    "                    distance.append(dis)\n",
    "    #                 progress_mini.update(1)\n",
    "                    success[std_index] += 1\n",
    "\n",
    "                    #################### saving as the clean data \n",
    "                    data_df[f'cf_{index}_{std_fact}'] = sample[0:20]\n",
    "                    data_df[f'cb_{index}_{std_fact}'] = sample[20:40]\n",
    "                    data_df[f'cc_{index}_{std_fact}'] = pd.Series(sample[40:].tolist())\n",
    "                    ###################\n",
    "\n",
    "                    ###########################\n",
    "                    column_name = f'{index}_{std_index+1}'\n",
    "                    sample_df = pd.DataFrame(sample.tolist(), index = range(1, numOfZ+1), columns = [column_name])\n",
    "                    cond_df = pd.DataFrame(condition.tolist(), index = range(1, numOfZ+1), columns = [column_name])\n",
    "                    false_Data_Gen = false_Data_Gen.append(sample_df.T,  ignore_index = False)\n",
    "                    \n",
    "                    ###########################\n",
    "                    dict_data['index'] = index\n",
    "                    dict_data['std_index'] = std_index\n",
    "                    dict_data['max_est'] = max_est\n",
    "                    dict_data['impact'] = np.linalg.norm(sample)\n",
    "                    dict_data['sample'] = sample.tolist()\n",
    "                    \n",
    "                    #dict_data['std_index'] = std_index\n",
    "                    final_Data = pd.concat([final_Data, pd.DataFrame(dict_data.values()).T], axis = 0)\n",
    "\n",
    "                    ###########################\n",
    "                    #print(\"Clean\")\n",
    "    #                 ###########################\n",
    "                    break\n",
    "\n",
    "    #             sample_init = Z_est_init[1:].copy() \n",
    "                #print(\"Repeating \", Noisy_index_actu.size)\n",
    "                sample_init = sample.copy()   \n",
    "\n",
    "                #saving sample and going for the next run\n",
    "    #             sample = Z_est_init[1:].copy()\n",
    "\n",
    "    false_Data_Gen_Est.append(false_Data_Gen)\n",
    "    success_rates.append(success)\n",
    "print(\"Done\")\n",
    "final_Data.columns = ['index', 'std_index', 'max_est', 'impact', 'sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_Data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gantype = 'cWGAN-Sen' if cond_type == 'Sensor' else 'cWGAN-Sub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4.5,4))\n",
    "\n",
    "fnt= '17.5'\n",
    "markers=['v','o','<','>','*','o','s','v','o','s']\n",
    "linestyles=['--','-.','-.','--','-.','-','-.','-','--','-.','-']\n",
    "for i,y in enumerate(np.array(success_rates)):\n",
    "    if i == -1:\n",
    "        continue\n",
    "    plt.plot(std_factors, y/cond_sample.shape[0]*100, marker = markers[i], linestyle = linestyles[i], markersize = '7.5', label = max_est_list[i])\n",
    "plt.title(f\"{gantype} with Different $\\gamma$\", fontsize = fnt)\n",
    "plt.xlabel(\"Threshold Factor, $\\lambda$\", fontsize = fnt)\n",
    "plt.xticks(fontsize = fnt)\n",
    "plt.yticks(fontsize = fnt)\n",
    "plt.ylabel(\"SR (%)\",  fontsize = fnt)\n",
    "# plt.legend(fontsize = fnt, framealpha = 0.5)\n",
    "plt.legend(bbox_to_anchor=(.05, 0.185), loc='best', ncol = 3, borderaxespad=0., fontsize = '13.5', framealpha =0.75) #bbox_to_anchor=(1.02, 0.95),\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//SR_{Model}_{cond_type}.jpg', dpi=350)\n",
    "plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//SR_{Model}_{cond_type}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = pd.DataFrame([])\n",
    "for max_iter in max_est_list:\n",
    "    for std_index in range(len(std_factors)):\n",
    "        #print(max_iter, std_index)\n",
    "        \n",
    "        filter1 = final_Data['max_est'] == max_iter\n",
    "        filter2 = final_Data['std_index'] == std_index\n",
    "        filtered_Data = final_Data.where(filter1 & filter2).dropna()\n",
    "#         print(filtered_Data.shape)\n",
    "        #### Genereating sens Condtion ###\n",
    "        ######################\n",
    "        filtered_samples = np.array([x for x in filtered_Data['sample'].values])\n",
    "        filtered_condSen = filtered_samples*0\n",
    "        filtered_condSen[filtered_samples != 0] = 1\n",
    "#         print(filtered_condSen)\n",
    "        if cond_type == 'Sensor':\n",
    "            target_cond = filtered_condSen.copy()\n",
    "        else:\n",
    "            #########################\n",
    "            sensor_ids_list = []\n",
    "            for filsen in filtered_condSen:\n",
    "                sensor_ids = np.where(filsen == 1)[0]+1\n",
    "                sensor_ids_list.append(sensor_ids)\n",
    "\n",
    "            filtered_condSub =  assignClusterID_v2 (sensor_ids_list, Topo, NumOfBuses=numOfBuses, NumOfLines = numOfLines)\n",
    "    #         print(filtered_condSub)\n",
    "            target_cond = filtered_condSub.copy()\n",
    "\n",
    "        #######################\n",
    "        filtered_cond = cond_sample[filtered_Data['index'].astype(int).values]\n",
    "        #####################\n",
    "        metric = {}\n",
    "        metric['Compliance'] = precision_score(filtered_cond.flatten(), target_cond.flatten())\n",
    "        metric['Utilization'] = recall_score(filtered_cond.flatten(), target_cond.flatten())\n",
    "        metric['max_est'] = max_iter \n",
    "        metric['std_index'] = std_index\n",
    "        metric_df = pd.concat([metric_df, pd.DataFrame(metric.values()).T], axis = 0)\n",
    "metric_df.columns = metric.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df_org = metric_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,6):\n",
    "metric_df = metric_df_org.copy()\n",
    "metric_df = metric_df[metric_df['max_est'] == 5]\n",
    "metric_df\n",
    "ax = plt.figure(figsize = (4.5,4))\n",
    "# ax.set_facecolor('white')\n",
    "# ax.set_bgcolor(\"white\")\n",
    "plt.plot(std_factors, metric_df['Compliance'], marker= 'p', markersize = '10', label = 'Compliance')\n",
    "plt.plot(std_factors, metric_df['Utilization'], marker ='*', markersize = '12', label = 'Utilization')\n",
    "# plt.title(f'Compliance-Utilization of {gantype}', fontsize = '16')\n",
    "plt.title(f\"{gantype} with Different $\\gamma$\", fontsize = fnt)\n",
    "\n",
    "plt.xlabel(\"Threshold Factor, $\\lambda$\", fontsize = fnt)\n",
    "plt.xticks(fontsize = fnt)\n",
    "plt.yticks(fontsize = fnt)\n",
    "plt.ylabel(\"Score\",  fontsize = fnt)\n",
    "# plt.legend(fontsize = fnt, framealpha = 0.5)\n",
    "plt.legend(loc='best', ncol = 1, borderaxespad=0., fontsize = fnt, framealpha = 0.25) #bbox_to_anchor=(1.05, 1), \n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//Pre_Rec_{cond_type}.jpg', dpi=350)\n",
    "plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//Pre_Rec_{cond_type}.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
